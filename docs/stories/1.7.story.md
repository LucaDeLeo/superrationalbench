# Story 1.7: Document Pilot Results and Limitations

## Status
Ready for Review

## Story
**As a** research analyst preparing the pilot handoff,
**I want** to capture the cooperation pilot findings, thresholds, and limitations in a lightweight report,
**so that** stakeholders can judge pilot success and schedule follow-up work without rerunning analyses.

## Acceptance Criteria
1. A `results/cooperation/pilot-summary.md` file (or CLI-generated equivalent) records total trials, decision coverage, effect sizes, pilot warnings, and the manifest path pulled from the latest analyzer output so QA can confirm the 120+ trial and ≥95% coverage thresholds at a glance. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#hour-5-documentation][Source: prd/pilot-study-scope-4-5-hour-implementation.md#success-criteria-for-pilot][Source: architecture/source-tree.md#New File Organization]
2. The summary highlights descriptive cooperation differences and interaction patterns using the actual analyzer metrics while explicitly constraining claims to those allowed for the pilot (practical effects, preliminary interaction evidence) and flagging disallowed statistical guarantees. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#what-you-can-claim][Source: prd/pilot-study-scope-4-5-hour-implementation.md#cannot-claim]
3. A limitations section documents scope constraints (runs per condition, model roster, single domain) and recommended follow-up actions so stakeholders understand how the pilot differs from the full program. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#sample-size--scope][Source: prd/pilot-study-scope-4-5-hour-implementation.md#hour-5-documentation]
4. README (or equivalent operator doc) gains a "Pilot Reporting" note that links the `run-pilot` → `analyze-pilot` workflow to the new summary file and states how to rerun the report if inputs change, keeping the 4-5 hour timeline visible. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#implementation-timeline][Source: architecture/quick-start-valid-experiment-in-4-5-hours.md#quick-start-valid-experiment-in-4-5-hours]

## Tasks / Subtasks
- [x] Task 1 (AC: 1): Extend the analyzer/report generator
  - [x] Teach `analyze-pilot` to emit a Markdown summary alongside the existing JSON, sourcing totals, coverage, and effect sizes from the generated statistics while including any threshold warnings and the manifest reference. [Source: docs/stories/1.6.story.md#Dev Notes][Source: architecture/component-architecture-simplified.md#Simple Flow]
  - [x] Default the Markdown output path to `results/cooperation/pilot-summary.md` and respect the existing `--output` flag so artifacts stay co-located with pilot data. [Source: architecture/source-tree.md#New File Organization][Source: docs/stories/1.6.story.md#Dev Notes]
- [x] Task 2 (AC: 1, 2, 3): Author the pilot results narrative
  - [x] Populate sections for data quality checks, cooperation findings, and interaction notes using the analyzer metrics while staying within the allowed claim boundaries. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#hour-5-documentation][Source: prd/pilot-study-scope-4-5-hour-implementation.md#what-you-can-claim]
  - [x] Document pilot limitations (sample size, model count, domain scope) and enumerate recommended next actions when metrics fall below thresholds. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#sample-size--scope][Source: prd/pilot-study-scope-4-5-hour-implementation.md#success-criteria-for-pilot]
- [x] Task 3 (AC: 2, 3, 4): Update operator documentation
  - [x] Add a "Pilot Reporting" section to README (or equivalent) that summarizes the `run-pilot` → `analyze-pilot` → summary workflow, notes rerun instructions, and calls out the new Markdown artifact location. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#implementation-timeline][Source: architecture/quick-start-valid-experiment-in-4-5-hours.md#quick-start-valid-experiment-in-4-5-hours]
  - [x] Mention the approved claim boundaries and limitations so operators share results responsibly. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#what-you-can-claim][Source: prd/pilot-study-scope-4-5-hour-implementation.md#cannot-claim]

## Dev Notes

### Previous Story Insights
- `analyze-pilot` already outputs pooled cooperation metrics, confidence intervals, and threshold warnings, so the summary should reuse that JSON payload and keep manifest references intact. [Source: docs/stories/1.6.story.md#Acceptance Criteria][Source: docs/stories/1.6.story.md#Dev Notes]
- The report generator lives in `src/analysis/` and honors `--output` plus `--silent`, so any summary writer must attach to that module instead of creating a parallel CLI path. [Source: docs/stories/1.6.story.md#Dev Notes]

### Data Models
- Pilot artifacts rely on `GameResult` records with `model`, `condition`, `decision`, `response`, and `timestamp`, and analysis focuses on symmetry and coupling factors only; summaries should echo those factors when describing findings. [Source: architecture/data-models-simple-types.md#Minimal Types You Need][Source: prd/pilot-study-scope-4-5-hour-implementation.md#simplified-22-factorial-not-222]

### API Specifications
No specific guidance found in architecture docs.

### Component Specifications
- The documentation step extends the simple flow’s final stage by adding human-readable reporting after statistics are computed, so plug into step 6 rather than altering the experiment loop. [Source: architecture/component-architecture-simplified.md#Simple Flow]

### File Locations
- Persist the Markdown summary inside `results/cooperation/` alongside manifests and analyzer JSON to respect the prescribed directory layout. [Source: architecture/source-tree.md#New File Organization]

### Testing Requirements
- Follow the manual testing guidance: rerun `analyze-pilot` against an existing manifest, inspect the Markdown summary for correct metrics and warnings, and confirm paths update when `--output` is overridden. [Source: architecture/testing-skip-for-now.md#Manual Testing Only]

### Technical Constraints
- Keep to the Bun/TypeScript stack without adding packages and reuse existing CLI utilities when emitting the summary. [Source: architecture/tech-stack-use-whats-already-there.md#Dont Add Anything New][Source: architecture/coding-standards-minimal.md#Just Make It Work]
- Track the pilot thresholds (≥120 trials, ≥95% coverage, effect size ≥0.5) and surface warnings clearly so documentation mirrors the success criteria. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#success-criteria-for-pilot]

### Project Structure Notes
- Co-locate the Markdown summary with the analyzer JSON outputs and manifest to keep QA artifacts together for downstream review. [Source: architecture/source-tree.md#New File Organization][Source: docs/stories/1.6.story.md#Project Structure Notes]

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-22 | 0.1 | Initial draft | Bob (SM) |

## Dev Agent Record

### Agent Model Used

- GPT-5 Codex

### Debug Log References

- n/a

### Completion Notes List

- Added Markdown summary generator that reuses analyzer metrics to surface totals, coverage, effect sizes, manifest path, and threshold warnings with pilot-scope caveats.
- README now links the run-pilot -> analyze-pilot loop to the new `results/cooperation/pilot-summary.md` artifact and reiterates rerun + claim boundaries for the 4-5 hour workflow.
- Manual `analyze-pilot` runs confirm Markdown output defaults under `results/cooperation/` and respects `--output`, exiting non-zero when warnings fire as expected.
- Unit coverage extended via `analyze-pilot` CLI test to assert Markdown emission, manifest reference, and warning propagation.

### File List

- README.md
- src/analysis/report-generator.ts
- src/cooperation/__tests__/analyze-pilot-cli.test.ts

## QA Results

### Review Date: 2025-09-22

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
- Analyzer workflow now emits a Markdown pilot summary co-located with the JSON analysis, surfacing trial totals, coverage, effect sizes, and warnings exactly as the ACs require.
- README gains a "Pilot reporting" note that ties the `run-pilot` → `analyze-pilot` loop to the new artifact and documents rerun guidance plus claim boundaries, keeping the 4-5 hour flow explicit.
- CLI tests now cover both warning and success paths, verifying Markdown creation, exit codes, and manifest path formatting.

### Refactoring Performed
- **File**: src/analysis/report-generator.ts
  - **Change**: Added `formatManifestPath` helper to emit repo-relative manifest paths in Markdown while falling back to absolute paths when needed.
  - **Why**: Keeps artifacts portable when shared outside the local filesystem snapshot.
  - **How**: Uses `path.relative` against `process.cwd()` with a safe fallback.

### Compliance Check
- Coding Standards: ✓ Aligned with existing Bun/TypeScript patterns; no new dependencies.
- Project Structure: ✓ Outputs stay under `results/cooperation/`; README section follows existing operator doc style.
- Testing Strategy: ✓ Added CLI test covering Markdown emission and threshold warnings.
- All ACs Met: ✓ Verified all four criteria via artifact inspection and README update.

### Improvements Checklist
- [x] Add a success-path CLI test that asserts Markdown generation when no warnings fire, to guard the PASS branch.
- [x] Emit manifest paths relative to the repo root in `pilot-summary.md` for portability when sharing artifacts.

### Security Review
- No new attack surface; analyzer still operates on local manifest data and writes Markdown to the existing results directory.

### Performance Considerations
- Markdown generation is linear in summary size and reuses existing analysis output; negligible overhead relative to the analyzer run.

### Files Modified During Review
- src/analysis/report-generator.ts
- src/cooperation/__tests__/analyze-pilot-cli.test.ts

### Gate Status
Gate: PASS → docs/qa/gates/1.7-document-pilot-results-and-limitations.yml
Risk profile: n/a
NFR assessment: n/a

### Recommended Status
[✓ Ready for Done]
