# Story 1.6: Analyze Pilot Cooperation Results

## Status
Done

## Story
**As a** research analyst preparing the CIRRUS-24 pilot report,
**I want** a lightweight analysis workflow that summarizes the cooperation trial outputs with reproducible metrics,
**so that** we can validate pilot success criteria within the 4-5 hour scope and decide whether to iterate.

## Acceptance Criteria
1. An analyzer consumes the persisted pilot trials and produces per-model and per-condition cooperation rates with standard errors and pooled effect sizes for symmetry and coupling, using only the simplified 2×2 factorial data. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#hour-4-basic-analysis][Source: prd/pilot-study-scope-4-5-hour-implementation.md#simplified-22-factorial-not-222][Source: architecture/component-architecture-simplified.md#Simple Flow]
2. The workflow emits 95% confidence intervals and a simple interaction check (difference-in-differences or equivalent) in a JSON summary saved under `results/cooperation/`, alongside optional console output for quick inspection. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#hour-4-basic-analysis][Source: architecture/source-tree.md#New File Organization][Source: architecture/quick-start-valid-experiment-in-4-5-hours.md#quick-start-valid-experiment-in-4-5-hours]
3. Summaries include automated warnings when total analyzed trials fall below 120, decision coverage drops under 95%, or effect sizes are under 0.5, aligning with the pilot success criteria. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#sample-size--scope][Source: prd/pilot-study-scope-4-5-hour-implementation.md#success-criteria-for-pilot][Source: architecture/checklist-results-report.md#Valid Experiment Requirements]
4. The analysis script documents its configuration (models, run counts, seed) in the output manifest and fails fast if required `GameResult` fields or condition factors are missing, keeping the workflow reproducible. [Source: docs/stories/1.5.story.md#Project Structure Notes][Source: architecture/data-models-simple-types.md#Minimal Types You Need][Source: architecture/coding-standards-minimal.md#Just Make It Work]

## Tasks / Subtasks
- [x] Task 1 (AC: 1, 2): Implement `src/analysis/statistical-analyzer.ts` to load pilot `GameResult` data, compute cooperation rates, standard errors, confidence intervals, and effect sizes using deterministic math utilities. [Source: architecture/source-tree.md#New File Organization][Source: architecture/quick-start-valid-experiment-in-4-5-hours.md#quick-start-valid-experiment-in-4-5-hours][Source: prd/pilot-study-scope-4-5-hour-implementation.md#hour-4-basic-analysis]
  - [x] Parse inputs from `results/cooperation/` batches generated in Story 1.5 and normalize by symmetry/coupling factors only. [Source: docs/stories/1.5.story.md#Project Structure Notes][Source: prd/pilot-study-scope-4-5-hour-implementation.md#simplified-22-factorial-not-222]
  - [x] Calculate pooled effect sizes for symmetry and coupling contrasts plus an interaction statistic and package them into a serializable summary object. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#hour-4-basic-analysis]
- [x] Task 2 (AC: 2, 3, 4): Write `src/analysis/report-generator.ts` (or extend existing CLI) to persist the JSON summary, print console highlights, and raise warnings when success criteria thresholds are not met. [Source: architecture/source-tree.md#New File Organization][Source: prd/pilot-study-scope-4-5-hour-implementation.md#success-criteria-for-pilot][Source: architecture/checklist-results-report.md#Valid Experiment Requirements]
  - [x] Attach configuration metadata (models, runs per condition, seed, timestamp) to the summary manifest for reproducibility. [Source: docs/stories/1.5.story.md#Project Structure Notes][Source: architecture/coding-standards-minimal.md#Just Make It Work]
  - [x] Emit structured warnings for low trial counts, coverage gaps, or small effect sizes so QA can decide on reruns quickly. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#success-criteria-for-pilot]
- [x] Task 3 (AC: 3, 4): Add a `bun run analyze-pilot` entry point that validates input completeness, invokes the analyzer/report generator, and exits non-zero on missing fields or thresholds breaches. [Source: architecture/infrastructure-keep-it-simple.md#just-run-locally][Source: architecture/coding-standards-minimal.md#Just Make It Work][Source: architecture/tech-stack-use-whats-already-there.md#Dont Add Anything New]
  - [x] Ensure the command reuses existing CLI plumbing from Story 1.5 instead of creating a new binary. [Source: docs/stories/1.5.story.md#Dev Notes]
  - [x] Document usage in README alongside the pilot runner so operators can chain execution within the 4-5 hour window. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#implementation-timeline][Source: architecture/quick-start-valid-experiment-in-4-5-hours.md#quick-start-valid-experiment-in-4-5-hours]
- [x] Task 4 (AC: 2, 3): Perform manual verification by running the analyzer on a sample batch, confirming JSON output, console warnings, and that 95%+ coverage triggers success. [Source: architecture/testing-skip-for-now.md#Manual Testing Only][Source: prd/pilot-study-scope-4-5-hour-implementation.md#success-criteria-for-pilot]
  - [x] Log verification steps in `.ai/debug-log.md` for traceability. [Source: architecture/coding-standards-minimal.md#Just Make It Work]

## Dev Notes

### Previous Story Insights
- Pilot runner already writes trial batches and manifests under `results/cooperation/`, so the analyzer must reuse those locations rather than scanning new directories. [Source: docs/stories/1.5.story.md#Project Structure Notes]
- CLI plumbing for `run-pilot` enforces roster and seed defaults; extending that switch for analysis keeps the operator flow consistent. [Source: docs/stories/1.5.story.md#Dev Notes]

### Data Models
- `GameResult` records provide `model`, `condition`, `decision`, `response`, and `timestamp`; analysis should treat `symmetry` and `coupling` as the only active factors per the pilot scope. [Source: architecture/data-models-simple-types.md#Minimal Types You Need][Source: prd/pilot-study-scope-4-5-hour-implementation.md#simplified-22-factorial-not-222]

### API Specifications
No specific guidance found in architecture docs.

### Component Specifications
- Simplified experiment flow culminates in calculating cooperation rates after saving JSON outputs, so the analyzer extends step 6 of the documented pipeline. [Source: architecture/component-architecture-simplified.md#Simple Flow]

### File Locations
- Place analysis modules in `src/analysis/` and persist summaries within `results/cooperation/` to align with the prescribed source organization. [Source: architecture/source-tree.md#New File Organization]

### Testing Requirements
- Follow the manual testing guidance: run the analyzer against a single pilot batch, inspect JSON outputs, and verify warnings before treating it as done. [Source: architecture/testing-skip-for-now.md#Manual Testing Only]

### Technical Constraints
- Stay within the existing Bun/TypeScript toolchain without adding packages, and keep deterministic math tied to the documented seed and manifest fields. [Source: architecture/tech-stack-use-whats-already-there.md#Dont Add Anything New][Source: architecture/coding-standards-minimal.md#Just Make It Work]
- Pilot success criteria require ≥120 trials, ≥95% decision coverage, and effect sizes ≥0.5, so the analyzer must track these thresholds explicitly. [Source: prd/pilot-study-scope-4-5-hour-implementation.md#sample-size--scope][Source: prd/pilot-study-scope-4-5-hour-implementation.md#success-criteria-for-pilot]

### Project Structure Notes
- Reuse the manifest emitted by Story 1.5 to capture configuration details alongside analysis outputs, keeping QA artifacts co-located for follow-up documentation. [Source: docs/stories/1.5.story.md#Project Structure Notes]

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-22 | 0.1 | Initial draft | Quinn |
| 2025-09-22 | 1.0 | Implemented analyzer, reporting CLI, and documentation updates | James |

## Dev Agent Record

### Agent Model Used

- GPT-5 (Codex)

### Debug Log References

- .ai/debug-log.md (2025-09-22 analyzer dry-run validation and unit tests)

### Completion Notes List

- Built `src/analysis/statistical-analyzer.ts` to load pilot manifests, aggregate cooperation metrics, compute pooled factor effects, and flag threshold breaches with structured warnings.
- Added `src/analysis/report-generator.ts` and the `analyze-pilot` CLI path to persist JSON summaries, auto-select recent manifests, emit console highlights, and exit non-zero when criteria fail.
- Backfilled documentation, unit coverage, and manual dry-run logs across README and `.ai/debug-log.md` so operators can chain pilot runs into analysis safely.

### File List

- src/analysis/statistical-analyzer.ts
- src/analysis/report-generator.ts
- src/analysis/__tests__/statistical-analyzer.test.ts
- src/cooperation/index.ts
- README.md
- .ai/debug-log.md

## QA Results
- _Pending_

### Review Date: 2025-09-22

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
Implementation meets all acceptance criteria: the analyzer enforces condition validation, computes pooled cooperation metrics with 95% CIs, and surfaces threshold breaches so operators can act quickly. Optional console output and JSON persistence are wired through the CLI entry point.

### Refactoring Performed
No refactoring performed; review was advisory only.

### Compliance Check
- Coding Standards: ✓ No deviations observed.
- Project Structure: ✓ Analyzer and CLI updates live under the prescribed directories.
- Testing Strategy: ✓ Unit coverage plus manual log entries align with documented guidance.
- All ACs Met: ✓ Verified against ACs 1-4 via code + CLI review.

### Improvements Checklist
- [x] Add an integration test that exercises `bun run analyze-pilot` end-to-end and asserts non-zero exit on warnings. (src/cooperation/index.ts)
- [x] Expand analyzer tests to cover low-trial, low-coverage, and small-effect warning scenarios for regression safety. (src/analysis/__tests__/statistical-analyzer.test.ts)
- [x] Document the `--silent` flag behavior in the operator README so expectations about console output are explicit. (README.md)

### Security Review
No new attack surface introduced; analyzer operates on local pilot artifacts with strict schema validation.

### Performance Considerations
Processing is linear in trial count and dominated by JSON I/O; acceptable for the pilot scale, with headroom for batching if future runs grow.

### Files Modified During Review
- src/analysis/__tests__/statistical-analyzer.test.ts
- src/cooperation/__tests__/analyze-pilot-cli.test.ts
- README.md

### Gate Status
Gate: PASS → docs/qa/gates/1.6-analyze-pilot-cooperation-results.yml
Risk profile: Not generated (not requested).
NFR assessment: Not generated (not requested).

### Recommended Status
[✓ Ready for Done] / [✗ Changes Required - See unchecked items above]
